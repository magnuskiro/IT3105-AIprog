\documentclass[titlepage]{article}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}

\author{Jan Alexander Bremnes\\Magnus Kir√∏}
\title{IT3105 - Ex 1\\Texas Hold'em}
\date{September 2011}

\begin{document}

    \maketitle
    \tableofcontents
    \pagenumbering{arabic}
    \newpage

\section{Introduction}
	This report describes our work done in exercise 1 of IT1305, which were to design and implement a poker-playing simulator for the game of Texas Hold'em. We decided to write the program in Python, since then we would not have to bother translating the supplied helper code into another language. This caused some minor problems and annoyances, as neither of us were very familiar with the Python language. Being an interpreted language, it runs a great deal slower than compiled languages such as Java and C/C++, so a phase 2 or 3 game consisting of hundreds of hands takes quite some time to run. Because of this, we limited the number of hands played in order to model the players, to 1000, with 10 players in each hand. The preflop rollout-table were calculated from a game with 40,000 hands, which took over 10 hours to simulate on a desktop computer with an Intel Core-i3. Not every single rule of Texas Hold'em is implemented, but where there are rules that we know are not followed properly, the report will say so. 
	
\section{Code Structure}
    The folder Texasholdem includes several modules, which comprise the main program. The subfolders /prefloprollout and /opponentmodeling, includes duplicates of some of these modules, in addition to modules necessary for the generation of the preflop rollout-table and opponent models. The folder /testrun contains the same modules as the main folder, but does not print debugging information, only the final results from each game. Also included in every folder, are some text files that supply example output files, which are smaller versions the files the rollout-table and opponent models were generated from. Please excuse the sometimes messy code, we did not have time to go through it all to do some cleaning, and sometimes patches were applied that fixed a bug that would only show up once in several hundred or thousand of hands.
	\subsection{Phase 1}
	    Phase 1 consists of a basic simulator for a k-player Texas Hold'Em, with k ranging from 2-10. No players are ever given the role as dealer, that is handled by the central play-poker module. Which of the players are given the role of small blind and big blind, is decided by randomly shuffling the list of players at the start of each hand and choosing the first player in the list as small blind and the second player as big blind. This does not guarantee that a player will have to wait for the blinds to go an entire round before finding himself in the role as small blind again, he may be small blind only two hands after last time, even though there are eight players in the game. We don't consider this to be of any noticable importance for our program. 
new-game.py starts the program, and let's the user choose the number of players, their starting amounts and how many hands shall be played. The only difference between play-poker.py and run-test.py, is that play-poker prints gameplay information, while run-test.py only prints the final results after having gone through the chosen number of rounds. Each game creates a list of player-objects, which are given a strategy, a player number, to keep track of who's who, and an initial amount of money. A table-object keeps track of the community cards, the amount of money in the pot, and the current bet. The game-state-object stores the players and table objects to allow easier passing of information between the different parts of the program. Every player has a strategy object, which it uses to decide on betting actions. The file strategy.py can be edited to change the existing strategies or add more. When a player is required to perform an action, the method evaluateHand, in the module betting.py. is called and depending on the power rating of the hand, and the players strategy, the player either folds, calls/checks or raises. The players uses the cards.py module to calculate the strength of their hands. In the pre-flop betting round, these are the only modules that are used. The modules cards.py is the helper code which was supplied to us, with a minor change added so that it's possible to remove cards from a card deck.
		
	\subsection{Phase 2}
	    In this phase of the project, we gave the players the ability to calculate the estimated probability that their hole cards will win the hand. A table of the winning probabilities for every equivalence class of hole cards, for every number of players, were generated by the code in the subfolder /prefloprollout. The modules roll-out*.py, where * = [1-3], simulate thousands of games for every possible combination of hole cards. We use three different modules because we thought it easier to just let the three run simultaneously on a multi core computer, rather than rewriting the code to use threads. The resulting file is then read by scanner.py, which counts the numbers of wins, losses and draws for every equivalence class and calculates the winning probabilities, which are stored in results.txt The players decide which equivalence class their hole cards belong to and use the table to find the estimated probability of winning the game with these cards. To calculate the probabilities after the flop, handstrength.py is used. This module calculates the probability of the player winning, when supplied with the hole cards, the community cards and the number of remaining players. It counts the number of times the players hole cards wins, draws or looses against every possible opponent. Using the formula in ai-poker-players.pdf, an estimate of the probability that the player wins against the given number of opponents, is returned.
		
    \subsection{Phase 3}
        In phase 3, functionality for creating context/action pairs, and analyzing these to create player models, were added. The information gathering was added to the play-poker.py module, and the resulting output was copied to a text file called contexts.txt. This file was analyzed by model-opponents.py and the resulting player models were stored in models.txt. The strategy.py module was updated to allow strategies to incorporate the models. 

\section{Logic behind the betting decisions}
	The poker players have different strategies for betting in each of the project phases. They differ from phase to phase, as the amount of information available to the players increase. Phase 1 has three different strategies; conservative, normal and aggressive. In Phase 2 and 3, we included a player that often bluffs.
	
	\subsection{Phase 1 - Primitive strategies, based on randomness and power ratings}
    In this phase, we made three different player strategies. These strategies are pretty basic, since the only information available to the players are their own hand strength. In the pre-flop round, betting behaviour is decided by weighted randomness. A conservative player will fold more often than an aggressive player, and an aggressive player will raise more often than a normal player, etc. Post-flop, the behaviour is determined solely by the power rating of the player performing the action. The conservative player folds when his power rating is lower than 4, calls on 5 & 6 and raises with a power rating of 7 or more. The normal player folds with a power rating less than 3, calls on 4 & 5, raise on 6 or greater. The aggressive player only folds on a power rating of 1, calls on 2 & 3, and raises on 4 or greater. 
	
	\subsection{Phase 2 - Strategies based on pre-flop roll-outs and hand strengths}
	    This phase of the project, required the calculation of a preflop rollout-table, and adding a hand strength calculator, both of which the Phase 2 players use to influence betting behaviour. The rollout-table is used prior the the flop, and the hand strength calculator is used in the remaining betting rounds. We used the results from a simulation of 40,000 games for all numbers of opponents and all possible equivalence classes, to calculate the rollut-table. In the pre-flop betting round, the player finds it's hole cards in the table, retrives the probability of winning the hand and calculates the expected probability for each player. If you're a bystander, not knowing the hole cards of any of the players, then you would say that each player has equal probability of winning the hand, based on the information you have. This is what we think of as the expected probability. Each players expected probability (Ep), in percent, would be Ep = 100/no.players. When the player calculates its probability of winning the hand (Cp), it compares this to Ep, and will perform an action, which is determined by the players strategy, and if Cp is equal, larger or smaller than Ep. A normal player, for example, will raise if it's Cp is equal to or larger than Ep, and fold if it's less than Ep/2. Aggressive players add an offset of 5-(potodds/2) to their Cp, and conservative players subtract this same offset. In addition to the three player classes from phase 1, we added a bluffer-class, which behaves as a regular player, but when a certain game state is reached, it will start bluffing. When a player bluffs, it calls or raises all the way to showdown. The bluffer class did not participate in any of the five different 1000-hand runs in this phase, nor in phase 3
		
	\subsection{Phase 3 - Strategies that include opponent modelling}
	    Unfortunately, within the project time frame, we were not able to implement the Phase 3 player strategies. But we will include a short description of how we inteded to do it. \\ The Phase 3 players would use the same rules as Phase 2 players, but they would also take into account the estimated hand strenghts of their opponents. These estimations are retrieved from the player models, which are stored as a text file that is loaded into memory when the program starts. The player models were generated from the context/action-pairs gathered over a 1000-hand, 10-players game. See models.txt. After the blinds, for every betting round, each player would record the current context, and its action. When it's the next players turn, he would look at the context/action pair that the previous player stored, look it up in that players model, and retrieve the expected hand strength. It would then store a new context with its own action, along with the expected hand strength of the previous player. Each player then uses the information stored by previous players, and adds new information to be used by the next player.
		
\section{The Opponent Modeling}
    In phase 3 we implemented functionality for basic opponent modeling. Every player object has a dictionary used to store context/action-pairs in. For every action a player performs, a new context/action pair is stored in that players dictionary. When the hand is finished, the context/action-pairs of players that have folded are discarded, and those who made it to showdown are analyzed. For every context/action-pair of the current hand, the players hand strength, at the time of play when the context was recorded, is calculated and stored in a second dictionary in the respective player object. The use of dictionaries ensures that only unique context/action-pairs are stored; the context/action pairs are used as the key, and the list of hand strengths as the value. The player models are stored in a text file, and is loaded into memory when the game starts. The player models were generated from the context/action-pairs gathered over a 200-hand, 10-players game, in which all the players were given a Phase 2 strategy. Too late in the project, we decided to create models from a 1000-hand game, but our system crashed (a lovely example of kernel panic) halfway through, and left us with no usable data, and not enough time to do a re-run. The code for the context/action gathering is not optimized in any way, so it is a bit slow running. When used in Phase 3 to compare strategies from all three phases, no models for Phase 1 players are used, as they do not exist. The game information used to define a context is the betting round (pre-flop = 1, post-flop = 2 ... ), how many players remain, and the pot odds. This is the only information used by the players to decide their behaviour, so using more information to define the context, such as number of raises, will create more contexts, and less useful player models. 
		
\section{Remarks}
    When there is a draw, and the pot is split between players, if the pot is not a multiple of the number of players it is split between, the total amount of money in the game will decrease. For example, if there are three winners, and the pot is 3002, then each player will win 1000, and the amount of money in play is reduced with 2. Because of this, there will sometimes be a small difference between the total amount the players started with, and the total amount they end up with.
    		
\section{Discussion of strategies}
    As can be seen from the tables containing the results from five 1000-hand runs from each completed project phase, some strategies were more successful than others. In phase 1, we can see that the aggressive players performed the best in all five 1000-hand games. In fact, only in one of the games did a conservative player win any money, but he was still beaten by the aggressive players. The normal players always ended up with great losses. The aggressive player probably performs the best because he will raise on hands normal and conservative players would fold on, so even if a conservative player had a better hand than the aggressive one, it might be that he folded, leaving the aggressive player to claim the prize. \\ \\ In phase 2, we see that the phase 1 players were easily beaten by the phase 2 players, which was to be expected since they have access to more information about the game, and change their behaviour accordingly. Among the Phase 2 players, there are some variations from game to game in regard to which player strategy performed the best, but overall the conservative and normal player beats the aggressive one. If we sum up the Phase 2 players winnings, we get that the normal player won a total of 287171, the conservative won 269858 and the aggressive 220910. They're quite close, but wee see that the normal player strategy was the best one in the long run. Aggressive players will sometimes loose large amounts of money, as they take bigger risks while playing, but they will also win large amounts. Conservative players take few risks, and will only stay in the game until the end if they're confident that they will win. This will keep them from loosing too much, but will also only award them when they have strong hands. Normal players do a little bit of both, they take some risks, but not too many, and this will reward them in the long run

\section{Player winnings tables}
    A = Aggressive player, C = Conservative player, N = normal player
	\subsection{Phase-1 player winnings table}
		5-players, 1000-games, 100-start money \\
		\begin{matrix}
		  	\\
			game set:&      1  &      2  &      3 &      4 &      5 \\
			Player 0 (N) & -15610  & -15560  & -25060 & -16880 & -24190 \\
			Player 1 (A) &  44410  &  36660  &  44530 &  41630 &  40960 \\
			Player 2 (A) &  24550  &  32180  &  31260 &  44960 &  39510 \\
			Player 3 (C) & -15760  & -17220  & -18240 & -24920 &  17210 \\
			Player 4 (C) & -22560  & -16220  & -10380 & -24140 & -19120 \\
            Player 5 (N) & -14430  & -19240  & -21510 & -20050 & -19350 \\
		\end{matrix}

	\subsection{Phase-2 player winnings table}
		5-players, 1000-games, 100-start money \\
		Player 0,1 and 3 are Phase 1 players \\
		\begin{matrix}
			\\
			game set:    &     1  &     2  &     3 &     4 &     5 \\
			Player 0 (N) &  -76470  &  -78260  & -76080  &  -79980 & -75100 \\
			Player 1 (A) &  -31084  &  -34787  & -34630  &  -37710 & -34414 \\
			Player 2 (C) &  -40166  &  -38621  & -82490  &  -20341 & -34820 \\
			Player 3 (A) &  41059   &  40706   & 63820   &   35916 & 39409 \\
			Player 4 (C) &  54729   &  42213   & 73950   &   53413 & 45553 \\
            Player 5 (N) &  52527   &  69346   & 56030   &   49299 & 59969 \\
		\end{matrix}
		
\section{Conclusion}
    We were unfortunately only able to complete two of the three phases of the project, we were not able to fully implement the phase 3 strategies, and therefor we were not able to do new test runs comparing Phase 3 players to Phase 1 and Phase 2 players. We managed to implement functionality for creating player models, so it would not require much work to complete phase 3. The test runs showed us that the Phase 2 players performed much better than Phase 1 players, and we intend to complete phase 3 to see if the opponent modeling improved player performance even more.



\end{document}
